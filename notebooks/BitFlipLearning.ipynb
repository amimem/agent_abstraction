{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import pylab as pl\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:  # Python 3.x\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heler Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary2index(var):\n",
    "    if var.ndim == 1:\n",
    "        return np.sum(np.power(2, np.arange(len(var))) * var).astype(int)\n",
    "    elif var.ndim == 2:\n",
    "        return np.sum(\n",
    "            np.power(2, np.arange(var.shape[0]))[:, np.newaxis] * var, axis=0\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        print(\"why more than 2 dimensions?\")\n",
    "\n",
    "\n",
    "def index2binary(var, n_all_agents):\n",
    "    if np.isscalar(var):\n",
    "        return (var & (1 << np.arange(n_all_agents))) > 0\n",
    "    elif var.ndim == 1:\n",
    "        return (var[np.newaxis, :] & (1 << np.arange(n_all_agents)[:, np.newaxis])) > 0\n",
    "    else:\n",
    "        print(\"why more than 1 dimensions?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bit Flip Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BF:\n",
    "    def __init__(self, n_actions, n_teams, team_size, policy_seed, avg_pairwise_correlation = 0.5, method=\"sum\"):\n",
    "        self.nA = n_actions\n",
    "        self.n_teams = n_teams\n",
    "        self.team_size = team_size\n",
    "        self.all_agents = n_teams * team_size + 1\n",
    "        self.all_other_agents = self.all_agents - 1\n",
    "        self.nS = self.nA ** (self.all_agents)\n",
    "        self.policy_seed = policy_seed\n",
    "        self.corr = avg_pairwise_correlation\n",
    "        self.method = method\n",
    "        self.policies = None\n",
    "        self.rng = np.random.default_rng(self.policy_seed)\n",
    "        self.agent_1_policy = self.rng.integers(0, 2, 2 ** (self.all_agents), dtype=bool)\n",
    "        self.get_other_agent_policies(gen_type=self.method)\n",
    "\n",
    "    def get_other_agent_policies(\n",
    "        self,\n",
    "        gen_type=\"sum\",\n",
    "        ):\n",
    "        self.policies = np.zeros(self.nS, dtype=int)\n",
    "\n",
    "        rng = np.random.default_rng(self.policy_seed)\n",
    "        agent_indices_bool = np.zeros(self.all_other_agents, dtype=bool)\n",
    "\n",
    "        rho_normaldist = np.sin(np.pi / 2 * self.corr)\n",
    "        batch_size = np.min((int(2 ** 25), self.nS))\n",
    "        tmp_policies = np.zeros((self.all_other_agents, batch_size), dtype=bool)\n",
    "        for batch_idx in range(int(self.nS / batch_size)):\n",
    "            for team in range(n_teams):\n",
    "                agent_indices = range(team * self.team_size, (team + 1) * self.team_size)\n",
    "                agent_indices_bool[agent_indices] = True\n",
    "\n",
    "                if (\n",
    "                    gen_type == \"mix\"\n",
    "                ):  # Bernoulli mixture of independent and identical binary RVs\n",
    "                    is_same = self.corr > self.rng.random(batch_size)\n",
    "                    n_same = np.sum(is_same)\n",
    "                    n_diff = batch_size - n_same\n",
    "                    tmp_policies[np.ix_(agent_indices_bool, is_same)] = self.rng.integers(\n",
    "                        0, 2, n_same\n",
    "                    )[np.newaxis, :]\n",
    "                    tmp_policies[np.ix_(agent_indices_bool, ~is_same)] = self.rng.integers(\n",
    "                        0, 2, [self.team_size, n_diff]\n",
    "                    )\n",
    "                elif (\n",
    "                    gen_type == \"sum\"\n",
    "                ):  # signed sum of independent and identical normal RVs\n",
    "                    tmp_policies[agent_indices_bool, :] = (\n",
    "                        np.sqrt(1 - rho_normaldist)\n",
    "                        * rng.normal(size=(self.team_size, batch_size))\n",
    "                        + np.sqrt(rho_normaldist)\n",
    "                        * rng.normal(size=batch_size)[np.newaxis, :]\n",
    "                    ) > 0\n",
    "                else:\n",
    "                    print(\"choose sum or mix\")\n",
    "\n",
    "                agent_indices_bool[agent_indices] = False\n",
    "            self.policies[\n",
    "                batch_idx * batch_size: (batch_idx + 1) * batch_size\n",
    "            ] = binary2index(\n",
    "                np.vstack(\n",
    "                    (\n",
    "                        self.agent_1_policy[\n",
    "                            batch_idx * batch_size: (batch_idx + 1) * batch_size\n",
    "                        ],\n",
    "                        tmp_policies,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def get_reward(self, agent_index, state):\n",
    "        fraction = state.sum()/len(state)\n",
    "        reward = 1.0 / (fraction*int(state[agent_index]) + (1-fraction)*int(1-state[agent_index]))\n",
    "        return reward\n",
    "\n",
    "    def P(self, s, a):\n",
    "        next_state = self.policies[s]\n",
    "        next_state = index2binary(next_state, self.all_agents)\n",
    "        next_state[0] = a\n",
    "        prob = 1.0 # transitions are deterministic\n",
    "        reward = self.get_reward(agent_index=0, state=next_state) # get reward for agent 0\n",
    "        return prob, next_state, reward\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        # return state 0 by deafult\n",
    "        self.state = 0\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    V_history = []\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward in [env.P(state, a)]:\n",
    "                next_state = binary2index(next_state)\n",
    "                # print(\"next state index is \", next_state)\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    time_step = 0\n",
    "    \n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        time_step += 1\n",
    "        delta = 0\n",
    "\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value  \n",
    "\n",
    "        # save sweep \n",
    "        V_history.append(np.mean(V))\n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            print(\"converged\", time_step)\n",
    "            break\n",
    "        if time_step >= max_iterations:\n",
    "            print(\"max iterations reached\", time_step)\n",
    "            break\n",
    "\n",
    "        if time_step % 100 == 0:\n",
    "            print(\"time step \", time_step)\n",
    "            # print(V[:10])\n",
    "\n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros(env.nS, dtype='int')\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        # A[0] corresponds to action 0 and A[1] corresponds to action 1, therefore instead of the one-hot value we use the index of the best action\n",
    "        policy[s] = best_action\n",
    "    \n",
    "    return policy, V, V_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "seed_list = np.random.randint(0, 2**32, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for team_size, n_teams in [(2,2), (5,2)]:\n",
    "    result_dict[(team_size, n_teams)] = {}\n",
    "    for rho in [0.0, 0.5, 1.0]:\n",
    "        result_dict[(team_size, n_teams)][rho] = {}\n",
    "        for seed in seed_list:\n",
    "            print(\"team size \", team_size, \" n_teams \", n_teams, \" rho \", rho, \" seed \", seed)\n",
    "            result_dict[(team_size, n_teams)][rho][seed] = {}\n",
    "            env_sum = BF(n_actions=2, n_teams=n_teams, team_size=team_size, avg_pairwise_correlation=rho, policy_seed=seed, method=\"sum\")\n",
    "            env_mix = BF(n_actions=2, n_teams=n_teams, team_size=team_size, avg_pairwise_correlation=rho, policy_seed=seed, method=\"mix\")\n",
    "            policy_sum, V_sum, V_sum_hist = value_iteration(env_sum, discount_factor=0.99, max_iterations=10000)\n",
    "            result_dict[(team_size, n_teams)][rho][seed][\"sum\"] = [policy_sum, V_sum, V_sum_hist]\n",
    "            policy_mix, V_mix, V_mix_hist = value_iteration(env_mix, discount_factor=0.99, max_iterations=10000)\n",
    "            result_dict[(team_size, n_teams)][rho][seed][\"mix\"] = [policy_mix, V_mix, V_mix_hist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('result.p', 'wb') as fp:\n",
    "#     pickle.dump(result_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.p', 'rb') as fp:\n",
    "    result_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sum, ax_sum = plt.subplots(2, 3, figsize=(16, 12), sharex=True, sharey=True)\n",
    "fig_mix, ax_mix = plt.subplots(2, 3, figsize=(16, 12), sharex=True, sharey=True)\n",
    "\n",
    "for i, pair in enumerate(result_dict):\n",
    "    for j, rho in enumerate(result_dict[pair]):\n",
    "        ax_sum[i][j].set_title(\"team size {}, n_teams {}, rho {}\".format(pair[0], pair[1], rho))\n",
    "        ax_sum[i][j].set_xlabel(\"iterations\")\n",
    "        ax_sum[i][j].set_ylabel(\"mean V\")\n",
    "\n",
    "        ax_mix[i][j].set_title(\"team size {}, n_teams {}, rho {}\".format(pair[0], pair[1], rho))\n",
    "        ax_mix[i][j].set_xlabel(\"iterations\")\n",
    "        ax_mix[i][j].set_ylabel(\"mean V\")\n",
    "\n",
    "        for seed in result_dict[pair][rho]:\n",
    "            ax_sum[i][j].plot(result_dict[pair][rho][seed][\"sum\"][2], label=\"f{}\".format(seed))\n",
    "            ax_mix[i][j].plot(result_dict[pair][rho][seed][\"mix\"][2], label=\"f{}\".format(seed))\n",
    "            \n",
    "            # ax_sum[i][j].legend()\n",
    "            # ax_mix[i][j].legend()\n",
    "fig_sum.suptitle(\"sum\")\n",
    "fig_mix.suptitle(\"mix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sum.tight_layout()\n",
    "fig_sum.savefig(\"fig_sum.pdf\")\n",
    "fig_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_mix.tight_layout()\n",
    "fig_mix.savefig(\"fig_mix.pdf\")\n",
    "fig_mix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sum, ax_sum = plt.subplots(1, 2, figsize=(16, 8), sharex=True, sharey=True)\n",
    "fig_mix, ax_mix = plt.subplots(1, 2, figsize=(16, 8), sharex=True, sharey=True)\n",
    "colors = [\"blue\", \"green\", \"orange\", \"purple\", \"black\"]\n",
    "blue_patch = mpatches.Patch(color='blue', label='The blue data')\n",
    "green_patch = mpatches.Patch(color='green', label='The green data')\n",
    "orange_patch = mpatches.Patch(color='orange', label='The orange data')\n",
    "\n",
    "rho_list = result_dict[pair].keys()\n",
    "\n",
    "for i, pair in enumerate(result_dict):\n",
    "    ax_sum[i].set_title(\"team size {}, n_teams {}\".format(pair[0], pair[1]))\n",
    "    ax_sum[i].set_xlabel(\"log iterations\")\n",
    "    ax_sum[i].set_ylabel(\"negative normalized V\")\n",
    "\n",
    "    ax_mix[i].set_title(\"team size {}, n_teams {}\".format(pair[0], pair[1]))\n",
    "    ax_mix[i].set_xlabel(\"log iterations\")\n",
    "    ax_mix[i].set_ylabel(\"negative normalized V\")\n",
    "    \n",
    "    for j, rho in enumerate(result_dict[pair]):\n",
    "        for seed in result_dict[pair][rho]:\n",
    "            V_mix_hist = result_dict[pair][rho][seed][\"sum\"][2]\n",
    "            V_sum_hist = result_dict[pair][rho][seed][\"mix\"][2]\n",
    "            normalized_V_mix_hist = [(V_mix_hist[i]-V_mix_hist[0])/(V_mix_hist[-1] - V_mix_hist[0]) for i in range(len(V_mix_hist))]\n",
    "            normalized_V_sum_hist = [(V_sum_hist[i]-V_sum_hist[0])/(V_sum_hist[-1] - V_sum_hist[0]) for i in range(len(V_sum_hist))]\n",
    "            neg_log_norm_V_mix_hist = [-np.log(normalized_V_mix_hist[i]) for i in range(len(normalized_V_mix_hist))]\n",
    "            neg_log_norm_V_sum_hist = [-np.log(normalized_V_sum_hist[i]) for i in range(len(normalized_V_sum_hist))]\n",
    "            line_sum = ax_sum[i].plot(neg_log_norm_V_sum_hist, color=colors[j])\n",
    "            line_mix = ax_mix[i].plot(neg_log_norm_V_mix_hist, color=colors[j])\n",
    "            \n",
    "    ax_sum[i].set_xscale('log')\n",
    "    ax_mix[i].set_xscale('log')\n",
    "\n",
    "fig_sum.legend([blue_patch, green_patch, orange_patch], rho_list)\n",
    "fig_mix.legend([blue_patch, green_patch, orange_patch], rho_list)\n",
    "fig_sum.suptitle(\"sum\")\n",
    "fig_mix.suptitle(\"mix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sum.tight_layout()\n",
    "fig_sum.savefig(\"fig_time_sum.pdf\")\n",
    "fig_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_mix.tight_layout()\n",
    "fig_mix.savefig(\"fig_time_mix.pdf\")\n",
    "fig_mix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(16, 12), sharex=True, sharey=True)\n",
    "\n",
    "for i, pair in enumerate(result_dict):\n",
    "    for j, rho in enumerate(result_dict[pair]):\n",
    "        ax[i][j].set_title(\"team size {}, n_teams {}, rho {}\".format(pair[0], pair[1], rho))\n",
    "        ax[i][j].set_xlabel(\"seed\")\n",
    "        ax[i][j].set_ylabel(\"log of normalized timescale\")\n",
    "\n",
    "        timescales = []\n",
    "        for seed in result_dict[pair][rho]:\n",
    "            timescales.append(len(result_dict[pair][rho][seed][\"het\"][2]))\n",
    "            # ax[i][j].plot(result_dict[pair][rho][seed][\"het\"][2], label=\"f{}\".format(seed))\n",
    "            # ax_het[i][j].plot(result_dict[pair][rho][seed][\"het\"][2], label=\"f{}\".format(seed))\n",
    "            # ax[i][j].legend()\n",
    "            # ax_het[i][j].legend()\n",
    "        timescales.sort()\n",
    "        normalized_timescales = [(t-timescales[0])/(timescales[-1] - timescales[0]) for t in timescales]\n",
    "        log_norm_t = [np.log(t) for t in normalized_timescales]\n",
    "        # print(timescales)\n",
    "        print(normalized_timescales)\n",
    "        ax[i][j].plot(log_norm_t)\n",
    "\n",
    "\n",
    "# fig.suptitle(\"homogeneous\")\n",
    "# fig_het.suptitle(\"heterogeneous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bit Flip Environment (Old One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MABitFlipEnv:\n",
    "\n",
    "    def __init__(self, n_actions, n_agents, n_memory, n_teams, avg_pairwise_correlation = 0.5, is_het=False):\n",
    "        self.nA = n_actions\n",
    "        self.n_agents = n_agents\n",
    "        self.n_memory = n_memory\n",
    "        self.nS = self.nA ** (self.n_agents * self.n_memory)\n",
    "        self.nS_plus = self.nA * self.nS\n",
    "        self.state = None\n",
    "        self.n_teams = n_teams\n",
    "        self.is_het = is_het\n",
    "        self.avg_pairwise_correlation = avg_pairwise_correlation\n",
    "        self.policies = np.zeros((self.n_agents, self.nS),dtype=bool)\n",
    "        self.joint_action_seqs = []\n",
    "        self.calculate_policies()\n",
    "\n",
    "    def get_hom_policies(self):\n",
    "        team_size = int(self.n_agents/self.n_teams)\n",
    "        rng = np.random.default_rng(12345)\n",
    "        agent_indices_bool = np.zeros(self.n_agents,dtype=bool)\n",
    "        for team in range(self.n_teams):\n",
    "            agent_indices = range(team * team_size, (team+1) * team_size)\n",
    "            agent_indices_bool[agent_indices]=True\n",
    "            #joint actions for a group are assigned Bernoulli: {as same over the group, else random}\n",
    "            is_same = (self.avg_pairwise_correlation > rng.random(self.nS)) #TODO: add more than binary ations is_same=(avg_pairwise_correlation>np.random.rand(0, n_actions, n_states)) #joint actions for a group are assigned as same over the group, else random\n",
    "            n_same = np.sum(is_same)\n",
    "            n_diff = self.nS-n_same\n",
    "            self.policies[np.ix_(agent_indices_bool, is_same)] = np.random.randint(0, self.nA, n_same)[np.newaxis,:]\n",
    "            self.policies[np.ix_(agent_indices_bool, ~is_same)] = np.random.randint(0, self.nA, [team_size,n_diff])\n",
    "            agent_indices_bool[agent_indices] = False #reset\n",
    "\n",
    "    def get_het_policies(self):\n",
    "        team_size = int(self.n_agents/self.n_teams)\n",
    "        rng = np.random.default_rng(12345)\n",
    "        agent_indices_bool = np.zeros(self.n_agents, dtype=bool)\n",
    "        rho=np.sin(np.pi/2*self.avg_pairwise_correlation)\n",
    "        for team in range(self.n_teams):\n",
    "            agent_indices = range(team * team_size, (team+1) * team_size)\n",
    "            agent_indices_bool[agent_indices]=True\n",
    "            self.policies[agent_indices_bool, :] = ((np.sqrt(1 - rho)*rng.normal(size = (team_size, self.nS)) + np.sqrt(rho)*rng.normal(size = self.nS)[np.newaxis,:]) > 0)\n",
    "            agent_indices_bool[agent_indices] = False\n",
    "\n",
    "    def calculate_policies(self):\n",
    "        if self.is_het:\n",
    "            self.get_het_policies()\n",
    "        else:\n",
    "            self.get_hom_policies()\n",
    "\n",
    "    def get_reward(self, agent_index, s_plus):\n",
    "        fraction = s_plus.sum()/len(s_plus)\n",
    "        reward = 1.0 / (fraction*int(s_plus[agent_index]) + (1-fraction)*int(1-s_plus[agent_index]))\n",
    "        return reward\n",
    "\n",
    "    def P(self, s, a):\n",
    "        next_state = self.policies[:, s]\n",
    "        next_s_plus = np.insert(next_state, 0, a)\n",
    "        prob = 1.0 # transitions are deterministic\n",
    "        reward = self.get_reward(0, next_s_plus) # get reward for agent 0\n",
    "        return prob, next_s_plus, reward\n",
    "    \n",
    "    def step(self, a):\n",
    "        next_state = self.policies[:, self.state]\n",
    "        self.state = binary2index(next_state)\n",
    "        next_s_plus = np.insert(next_state, 0, a)\n",
    "        reward = self.get_reward(0, next_s_plus)\n",
    "        return next_s_plus, reward\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        # return state 0 by deafult\n",
    "        self.state = 0\n",
    "        return self.state"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "519c3646e46e87a9b4521f30b0c71a5bab07601a45b52f63f01adb46cf5a2090"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('marl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
