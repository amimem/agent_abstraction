{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import pylab as pl\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:  # Python 3.x\n",
    "    import pickle\n",
    "\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.signal import lfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "%load_ext line_profiler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heler Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary2index(var):\n",
    "    if var.ndim == 1:\n",
    "        return np.sum(np.power(2, np.arange(len(var))) * var).astype(int)\n",
    "    elif var.ndim == 2:\n",
    "        return np.sum(\n",
    "            np.power(2, np.arange(var.shape[0]))[:, np.newaxis] * var, axis=0\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        print(\"why more than 2 dimensions?\")\n",
    "\n",
    "\n",
    "def index2binary(var, n_all_agents):\n",
    "    if np.isscalar(var):\n",
    "        return (var & (1 << np.arange(n_all_agents))) > 0\n",
    "    elif var.ndim == 1:\n",
    "        return (var[np.newaxis, :] & (1 << np.arange(n_all_agents)[:, np.newaxis])) > 0\n",
    "    else:\n",
    "        print(\"why more than 1 dimensions?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bit Flip Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BF:\n",
    "    def __init__(self, n_actions, n_teams, team_size, policy_seed, avg_pairwise_correlation = 0.5, method=\"sum\"):\n",
    "        self.nA = n_actions\n",
    "        self.n_teams = n_teams\n",
    "        self.team_size = team_size\n",
    "        self.all_agents = n_teams * team_size + 1\n",
    "        self.all_other_agents = self.all_agents - 1\n",
    "        self.nS = self.nA ** (self.all_agents)\n",
    "        self.policy_seed = policy_seed\n",
    "        self.corr = avg_pairwise_correlation\n",
    "        self.method = method\n",
    "        self.policies = None\n",
    "        self.rng = np.random.default_rng(self.policy_seed)\n",
    "        self.agent_1_policy = self.rng.integers(0, 2, 2 ** (self.all_agents), dtype=bool)\n",
    "        self.get_other_agent_policies(gen_type=self.method)\n",
    "\n",
    "    def get_other_agent_policies(\n",
    "        self,\n",
    "        gen_type=\"sum\",\n",
    "        ):\n",
    "        self.policies = np.zeros(self.nS, dtype=int)\n",
    "\n",
    "        rng = np.random.default_rng(self.policy_seed)\n",
    "        agent_indices_bool = np.zeros(self.all_other_agents, dtype=bool)\n",
    "\n",
    "        rho_normaldist = np.sin(np.pi / 2 * self.corr)\n",
    "        batch_size = np.min((int(2 ** 25), self.nS))\n",
    "        tmp_policies = np.zeros((self.all_other_agents, batch_size), dtype=bool)\n",
    "        for batch_idx in range(int(self.nS / batch_size)):\n",
    "            for team in range(self.n_teams):\n",
    "                agent_indices = range(team * self.team_size, (team + 1) * self.team_size)\n",
    "                agent_indices_bool[agent_indices] = True\n",
    "\n",
    "                if (\n",
    "                    gen_type == \"mix\"\n",
    "                ):  # Bernoulli mixture of independent and identical binary RVs\n",
    "                    is_same = self.corr > self.rng.random(batch_size)\n",
    "                    n_same = np.sum(is_same)\n",
    "                    n_diff = batch_size - n_same\n",
    "                    tmp_policies[np.ix_(agent_indices_bool, is_same)] = self.rng.integers(\n",
    "                        0, 2, n_same\n",
    "                    )[np.newaxis, :]\n",
    "                    tmp_policies[np.ix_(agent_indices_bool, ~is_same)] = self.rng.integers(\n",
    "                        0, 2, [self.team_size, n_diff]\n",
    "                    )\n",
    "                elif (\n",
    "                    gen_type == \"sum\"\n",
    "                ):  # signed sum of independent and identical normal RVs\n",
    "                    tmp_policies[agent_indices_bool, :] = (\n",
    "                        np.sqrt(1 - rho_normaldist)\n",
    "                        * rng.normal(size=(self.team_size, batch_size))\n",
    "                        + np.sqrt(rho_normaldist)\n",
    "                        * rng.normal(size=batch_size)[np.newaxis, :]\n",
    "                    ) > 0\n",
    "                else:\n",
    "                    print(\"choose sum or mix\")\n",
    "\n",
    "                agent_indices_bool[agent_indices] = False\n",
    "            self.policies[\n",
    "                batch_idx * batch_size: (batch_idx + 1) * batch_size\n",
    "            ] = binary2index(\n",
    "                np.vstack(\n",
    "                    (\n",
    "                        self.agent_1_policy[\n",
    "                            batch_idx * batch_size: (batch_idx + 1) * batch_size\n",
    "                        ],\n",
    "                        tmp_policies,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def get_reward(self, agent_index, state):\n",
    "        fraction = state.sum()/len(state)\n",
    "        reward = 1.0 / (fraction*int(state[agent_index]) + (1-fraction)*int(1-state[agent_index]))\n",
    "        return reward\n",
    "\n",
    "    def P(self, s, a):\n",
    "        next_state = self.policies[s]\n",
    "        next_state = index2binary(next_state, self.all_agents)\n",
    "        next_state[0] = a\n",
    "        prob = 1.0 # transitions are deterministic\n",
    "        reward = self.get_reward(agent_index=0, state=next_state) # get reward for agent 0\n",
    "        return prob, next_state, reward\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        # return state 0 by deafult\n",
    "        self.state = 0\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    V_history = []\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward in [env.P(state, a)]:\n",
    "                next_state = binary2index(next_state)\n",
    "                # print(\"next state index is \", next_state)\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    time_step = 0\n",
    "    \n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        time_step += 1\n",
    "        delta = 0\n",
    "\n",
    "        # Update each state...\n",
    "        # TODO: randmized order\n",
    "        r_list = np.arange(env.nS)\n",
    "        np.random.shuffle(r_list)\n",
    "        for s in r_list:\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value  \n",
    "\n",
    "        # save sweep \n",
    "        V_history.append(np.mean(V))\n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            print(\"converged\", time_step, end=\"\\n\", flush=True)\n",
    "            break\n",
    "        if time_step >= max_iterations:\n",
    "            print(\"max iterations reached\", time_step, end=\"\\n\", flush=True)\n",
    "            break\n",
    "\n",
    "        # if time_step % 100 == 0:\n",
    "            # print(\"time step \", time_step)\n",
    "            # print(V[:10])\n",
    "\n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros(env.nS, dtype='int')\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        # A[0] corresponds to action 0 and A[1] corresponds to action 1, therefore instead of the one-hot value we use the index of the best action\n",
    "        policy[s] = best_action\n",
    "    \n",
    "    return policy, V, V_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "seed_list = np.random.randint(0, 2**32, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for team_size, n_teams in [(5,1), (10,1)]:\n",
    "    result_dict[(team_size, n_teams)] = {}\n",
    "    for rho in [0.0, 0.5, 1.0]:\n",
    "        result_dict[(team_size, n_teams)][rho] = {}\n",
    "        # TODO: remove :3 after testing\n",
    "        for seed in seed_list:\n",
    "            print(\"team size \", team_size, \" n_teams \", n_teams, \" rho \", rho, \" seed \", seed)\n",
    "            result_dict[(team_size, n_teams)][rho][seed] = {}\n",
    "            env_sum = BF(n_actions=2, n_teams=n_teams, team_size=team_size, avg_pairwise_correlation=rho, policy_seed=seed, method=\"sum\")\n",
    "            env_mix = BF(n_actions=2, n_teams=n_teams, team_size=team_size, avg_pairwise_correlation=rho, policy_seed=seed, method=\"mix\")\n",
    "            policy_sum, V_sum, V_sum_hist = value_iteration(env_sum, discount_factor=0.99, max_iterations=10000)\n",
    "            result_dict[(team_size, n_teams)][rho][seed][\"sum\"] = [policy_sum, V_sum, V_sum_hist]\n",
    "            policy_mix, V_mix, V_mix_hist = value_iteration(env_mix, discount_factor=0.99, max_iterations=10000)\n",
    "            result_dict[(team_size, n_teams)][rho][seed][\"mix\"] = [policy_mix, V_mix, V_mix_hist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sims(team_tuple, rho_list, seed_list):\n",
    "    result_dict = {}\n",
    "    for team_size, n_teams in team_tuple:\n",
    "        result_dict[(team_size, n_teams)] = {}\n",
    "        for rho in rho_list:\n",
    "            result_dict[(team_size, n_teams)][rho] = {}\n",
    "            for seed in seed_list:\n",
    "                # print(\"team size \", team_size, \" n_teams \", n_teams, \" rho \", rho, \" seed \", seed)\n",
    "                result_dict[(team_size, n_teams)][rho][seed] = {}\n",
    "                env_sum = BF(n_actions=2, n_teams=n_teams, team_size=team_size, avg_pairwise_correlation=rho, policy_seed=seed, method=\"sum\")\n",
    "                env_mix = BF(n_actions=2, n_teams=n_teams, team_size=team_size, avg_pairwise_correlation=rho, policy_seed=seed, method=\"mix\")\n",
    "                policy_sum, V_sum, V_sum_hist = value_iteration(env_sum, discount_factor=0.99, max_iterations=10000)\n",
    "                result_dict[(team_size, n_teams)][rho][seed][\"sum\"] = [policy_sum, V_sum, V_sum_hist]\n",
    "                policy_mix, V_mix, V_mix_hist = value_iteration(env_mix, discount_factor=0.99, max_iterations=10000)\n",
    "                result_dict[(team_size, n_teams)][rho][seed][\"mix\"] = [policy_mix, V_mix, V_mix_hist]\n",
    "                \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_sims([(2,2), (5,2)], [0.0, 0.5, 1.0], seed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many minutes do we need ? \n",
    "\n",
    "for n = 15 , a pair of runs (mix and run) takes about an hour ... ten seeds take about 10 hours, 3 rhos ... 30 hours ... \n",
    "\n",
    "\n",
    "for n = 14, a pair takes about 30 mins .... ten seeds 5 hours, 3 rhos 15 hours ...\n",
    "\n",
    "\n",
    "for n = 13, 7.5 hours\n",
    "\n",
    "\n",
    "for n = 12, 3.75 hours\n",
    "\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Memory usage is low ... about 50 mg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('result_rand_two_teams.p', 'wb') as fp:\n",
    "#     pickle.dump(result_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_rand_one_team.p', 'rb') as fp:\n",
    "    result_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "columns = set()\n",
    "data_sum = []\n",
    "data_mix = []\n",
    "\n",
    "for i, pair in enumerate(result_dict):\n",
    "    for j, rho in enumerate(result_dict[pair]):\n",
    "        rows.append(pair + (rho,))\n",
    "        for seed in result_dict[pair][rho]:\n",
    "            columns.add((seed,))\n",
    "            n_all_agents = pair[0] * pair[1] + 1\n",
    "            a = np.arange(0, 2**(n_all_agents))\n",
    "            b = index2binary(a, n_all_agents)\n",
    "            optimal_p = (b.sum(axis=0) < n_all_agents/2)\n",
    "            learned_p_mix= np.array(result_dict[pair][rho][seed][\"mix\"][0], dtype=bool)\n",
    "            learned_p_sum = np.array(result_dict[pair][rho][seed][\"sum\"][0], dtype=bool)\n",
    "\n",
    "            data_sum.append(np.mean(learned_p_sum == optimal_p))\n",
    "            data_mix.append(np.mean(learned_p_mix == optimal_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.array([i for (i,) in columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = np.array(rows)\n",
    "columns = np.array(columns)\n",
    "data_sum = np.array(data_sum).reshape(rows.shape[0], columns.shape[0])\n",
    "data_mix = np.array(data_mix).reshape(rows.shape[0], columns.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the vlaue of different states over the number of ones they contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# hide axes\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "\n",
    "ax.table(cellText=data_sum,\n",
    "        rowLabels=rows,\n",
    "        colLabels=columns,\n",
    "        loc='center')\n",
    "\n",
    "fig.suptitle(\"Sum\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"table_sum.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# hide axes\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "\n",
    "ax.table(cellText=data_sum,\n",
    "        rowLabels=rows,\n",
    "        colLabels=columns,\n",
    "        loc='center')\n",
    "\n",
    "fig.suptitle(\"Mix\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"table_mix.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sum, ax_sum = plt.subplots(2, 3, figsize=(16, 12), sharex=True, sharey=True)\n",
    "fig_mix, ax_mix = plt.subplots(2, 3, figsize=(16, 12), sharex=True, sharey=True)\n",
    "\n",
    "for i, pair in enumerate(result_dict):\n",
    "    for j, rho in enumerate(result_dict[pair]):\n",
    "        ax_sum[i][j].set_title(\"team size {}, n_teams {}, rho {}\".format(pair[0], pair[1], rho))\n",
    "        ax_sum[i][j].set_xlabel(\"number of ones\")\n",
    "        ax_sum[i][j].set_ylabel(\"converged V\")\n",
    "\n",
    "        ax_mix[i][j].set_title(\"team size {}, n_teams {}, rho {}\".format(pair[0], pair[1], rho))\n",
    "        ax_mix[i][j].set_xlabel(\"number of ones\")\n",
    "        ax_mix[i][j].set_ylabel(\"converged V\")\n",
    "\n",
    "\n",
    "\n",
    "        for seed in result_dict[pair][rho]:\n",
    "\n",
    "            V_sum = result_dict[pair][rho][seed][\"sum\"][1]\n",
    "            x = np.arange(len(V_sum))\n",
    "            num_agents = int(np.log2(len(V_sum)))\n",
    "            x_bin = index2binary(x, num_agents)\n",
    "            num_ones = x_bin.sum(axis=0)\n",
    "            counts = collections.Counter(num_ones)\n",
    "            counts_full_len = np.zeros(len(num_ones))\n",
    "            for k, v in counts.items():\n",
    "                counts_full_len[k] = v\n",
    "\n",
    "            # print(num_ones)\n",
    "            # print(counts)\n",
    "            # print(counts_full_len)\n",
    "\n",
    "            # break\n",
    "            \n",
    "            ax_sum[i][j].scatter(num_ones, V_sum, s=counts_full_len, label=\"seed {}\".format(seed))\n",
    "            V_mix = result_dict[pair][rho][seed][\"mix\"][1]\n",
    "            ax_mix[i][j].scatter(num_ones, V_mix, s=counts_full_len, label=\"seed {}\".format(seed))\n",
    "        \n",
    "fig_sum.suptitle(\"sum\")\n",
    "fig_mix.suptitle(\"mix\")\n",
    "\n",
    "fig_sum.tight_layout()\n",
    "fig_sum.savefig(\"fig_sum_ones_team_1.pdf\")\n",
    "\n",
    "fig_mix.tight_layout()\n",
    "fig_mix.savefig(\"fig_mix_ones_team_1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sum, ax_sum = plt.subplots(2, 3, figsize=(16, 12), sharex=True, sharey=True)\n",
    "fig_mix, ax_mix = plt.subplots(2, 3, figsize=(16, 12), sharex=True, sharey=True)\n",
    "\n",
    "for i, pair in enumerate(result_dict):\n",
    "    for j, rho in enumerate(result_dict[pair]):\n",
    "        ax_sum[i][j].set_title(\"team size {}, n_teams {}, rho {}\".format(pair[0], pair[1], rho))\n",
    "        ax_sum[i][j].set_xlabel(\"iterations\")\n",
    "        ax_sum[i][j].set_ylabel(\"mean V\")\n",
    "\n",
    "        ax_mix[i][j].set_title(\"team size {}, n_teams {}, rho {}\".format(pair[0], pair[1], rho))\n",
    "        ax_mix[i][j].set_xlabel(\"iterations\")\n",
    "        ax_mix[i][j].set_ylabel(\"mean V\")\n",
    "\n",
    "        for seed in result_dict[pair][rho]:\n",
    "            ax_sum[i][j].plot(result_dict[pair][rho][seed][\"sum\"][2], label=\"f{}\".format(seed))\n",
    "            ax_mix[i][j].plot(result_dict[pair][rho][seed][\"mix\"][2], label=\"f{}\".format(seed))\n",
    "            \n",
    "fig_sum.suptitle(\"sum\")\n",
    "fig_mix.suptitle(\"mix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sum.tight_layout()\n",
    "fig_sum.savefig(\"fig_sum.pdf\")\n",
    "fig_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_mix.tight_layout()\n",
    "fig_mix.savefig(\"fig_mix.pdf\")\n",
    "fig_mix.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sum, ax_sum = plt.subplots(1, 2, figsize=(16, 8), sharex=True, sharey=True)\n",
    "fig_mix, ax_mix = plt.subplots(1, 2, figsize=(16, 8), sharex=True, sharey=True)\n",
    "colors = [\"blue\", \"green\", \"orange\", \"purple\", \"black\"]\n",
    "blue_patch = mpatches.Patch(color='blue', label='The blue data')\n",
    "green_patch = mpatches.Patch(color='green', label='The green data')\n",
    "orange_patch = mpatches.Patch(color='orange', label='The orange data')\n",
    "\n",
    "rho_list = result_dict[pair].keys()\n",
    "\n",
    "for i, pair in enumerate(result_dict):\n",
    "    ax_sum[i].set_title(\"team size {}, n_teams {}\".format(pair[0], pair[1]))\n",
    "    ax_sum[i].set_xlabel(\"iteration\")\n",
    "    ax_sum[i].set_ylabel(\"log normalized V\")\n",
    "    ax_mix[i].set_title(\"team size {}, n_teams {}\".format(pair[0], pair[1]))\n",
    "    ax_mix[i].set_xlabel(\"iteration\")\n",
    "    ax_mix[i].set_ylabel(\"log normalized V\")\n",
    "    \n",
    "    for j, rho in enumerate(result_dict[pair]):\n",
    "        for seed in result_dict[pair][rho]:\n",
    "            V_mix_hist = result_dict[pair][rho][seed][\"sum\"][2]\n",
    "            V_sum_hist = result_dict[pair][rho][seed][\"mix\"][2]\n",
    "\n",
    "            normalized_V_mix_hist = [abs(V_mix_hist[i]-V_mix_hist[-1])/(V_mix_hist[-1] - V_mix_hist[0]) for i in range(len(V_mix_hist))]\n",
    "            normalized_V_sum_hist = [abs(V_sum_hist[i]-V_sum_hist[-1])/(V_sum_hist[-1] - V_sum_hist[0]) for i in range(len(V_sum_hist))]\n",
    "\n",
    "            line_sum = ax_sum[i].plot(normalized_V_mix_hist, color=colors[j])\n",
    "            line_mix = ax_mix[i].plot(normalized_V_sum_hist, color=colors[j])\n",
    "            \n",
    "    ax_sum[i].set_yscale('log')\n",
    "    ax_mix[i].set_yscale('log')\n",
    "\n",
    "fig_sum.legend([blue_patch, green_patch, orange_patch], rho_list)\n",
    "fig_mix.legend([blue_patch, green_patch, orange_patch], rho_list)\n",
    "fig_sum.suptitle(\"sum\")\n",
    "fig_mix.suptitle(\"mix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sum.tight_layout()\n",
    "fig_sum.savefig(\"fig_time_sum.pdf\")\n",
    "fig_sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_mix.tight_layout()\n",
    "fig_mix.savefig(\"fig_time_mix.pdf\")\n",
    "fig_mix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_policy(policy, env, max_iterations):\n",
    "    # policy_sum = result_dict[(team_size, n_teams)][rho][seed][\"sum\"][0]\n",
    "    reward_hist = []\n",
    "    state = env.reset()\n",
    "    for ix in range(max_iterations):\n",
    "        action = policy[state]   # get action from the policy\n",
    "        prob, next_state, reward = env.P(state, action)\n",
    "        reward_hist.append(reward)\n",
    "        state = binary2index(next_state)\n",
    "    # reward_dict[(team_size, n_teams)][rho][seed][\"sum\"] = reward_hist\n",
    "    return reward_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_dictionary = {}\n",
    "for i, pair in enumerate(result_dict):\n",
    "    reward_dictionary[pair] = {}\n",
    "    for j, rho in enumerate(result_dict[pair]):\n",
    "        reward_dictionary[pair][rho] = {}\n",
    "        for seed in result_dict[pair][rho]:\n",
    "            reward_dictionary[pair][rho][seed] = {}\n",
    "\n",
    "            policy_sum = result_dict[pair][rho][seed][\"sum\"][0]\n",
    "            policy_mix = result_dict[pair][rho][seed][\"mix\"][0]\n",
    "            env_sum = BF(n_actions=2, n_teams=pair[0], team_size=pair[1], avg_pairwise_correlation=rho, policy_seed=seed, method=\"sum\")\n",
    "            env_mix = BF(n_actions=2, n_teams=pair[0], team_size=pair[1], avg_pairwise_correlation=rho, policy_seed=seed, method=\"mix\")\n",
    "            reward_dictionary[pair][rho][seed][\"sum\"] = execute_policy(policy_sum, env_sum, max_iterations=1000)\n",
    "            reward_dictionary[pair][rho][seed][\"mix\"] = execute_policy(policy_mix, env_mix, max_iterations=1000)\n",
    "\n",
    "            # SUM RANDOM POLICY\n",
    "            np.random.seed(seed)\n",
    "            policy_rand = np.random.choice(([0, 1]), size = len(policy_sum))\n",
    "            reward_dictionary[pair][rho][seed][\"random_sum\"] = execute_policy(policy_rand,env_sum,max_iterations=1000)\n",
    "\n",
    "            # MIX RANDOM POLICY\n",
    "            np.random.seed(seed)\n",
    "            policy_rand = np.random.choice(([0, 1]), size = len(policy_mix))\n",
    "            reward_dictionary[pair][rho][seed][\"random_mix\"] = execute_policy(policy_rand,env_mix,max_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alt(rewards, discount):\n",
    "    \"\"\"\n",
    "    C[i] = R[i] + discount * C[i+1]\n",
    "    signal.lfilter(b, a, x, axis=-1, zi=None)\n",
    "    a[0]*y[n] = b[0]*x[n] + b[1]*x[n-1] + ... + b[M]*x[n-M]\n",
    "                          - a[1]*y[n-1] - ... - a[N]*y[n-N]\n",
    "    \"\"\"\n",
    "    r = rewards[::-1]\n",
    "    a = [1, -discount]\n",
    "    b = [1]\n",
    "    y = lfilter(b, a, x=r)\n",
    "    return y[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sum, ax_sum = plt.subplots(2, 3, figsize=(15, 7), sharex=True, sharey=True)\n",
    "fig_mix, ax_mix = plt.subplots(2, 3, figsize=(15, 7), sharex=True, sharey=True)\n",
    "colors = [\"blue\", \"green\", \"orange\", \"purple\", \"black\", \"pink\", \"cyan\", \"maroon\", \"yellow\", \"olive\"]\n",
    "blue_patch = mpatches.Patch(color='blue', label='The blue data')\n",
    "orange_patch = mpatches.Patch(color='orange', label='The orange data')\n",
    "green_patch = mpatches.Patch(color='green', label='The green data')\n",
    "purple_patch = mpatches.Patch(color='purple', label='The purple data')\n",
    "black_patch = mpatches.Patch(color='black', label='The black data')\n",
    "pink_patch = mpatches.Patch(color='pink', label='The pink data')\n",
    "cyan_patch = mpatches.Patch(color='cyan', label='The cyan data')\n",
    "maroon_patch = mpatches.Patch(color='maroon', label='The maroon data')\n",
    "yellow_patch = mpatches.Patch(color='yellow', label='The yellow data')\n",
    "olive_patch = mpatches.Patch(color='olive', label='The olive data')\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "for i, pair in enumerate(reward_dictionary):\n",
    "    for j, rho in enumerate(reward_dictionary[pair]):\n",
    "        ax_sum[i][j].set_title(\"team size {}, n_teams {}, rho {}\".format(pair[0], pair[1], rho))\n",
    "        ax_sum[i][j].set_xlabel(\"timesteps\")\n",
    "        ax_sum[i][j].set_ylabel(\"mean V\")\n",
    "\n",
    "        ax_mix[i][j].set_title(\"team size {}, n_teams {}, rho {}\".format(pair[0], pair[1], rho))\n",
    "        ax_mix[i][j].set_xlabel(\"timesteps\")\n",
    "        ax_mix[i][j].set_ylabel(\"mean V\")\n",
    "\n",
    "        reward_matrix_optimal_sum = []\n",
    "        reward_matrix_optimal_mix = []\n",
    "        reward_matrix_random_sum = []\n",
    "        reward_matrix_random_mix = []\n",
    "\n",
    "        for seed in reward_dictionary[pair][rho]:\n",
    "            # discounter = np.array([gamma**i for i in range(len(reward_dictionary[pair][rho][seed][\"sum\"]))])\n",
    "            # reward_matrix_optimal_sum.append(np.dot(reward_dictionary[pair][rho][seed][\"sum\"], discounter))\n",
    "            # reward_matrix_optimal_mix.append(np.dot(reward_dictionary[pair][rho][seed][\"mix\"], discounter))\n",
    "            # reward_matrix_random_sum.append(np.dot(reward_dictionary[pair][rho][seed][\"random_sum\"], discounter))\n",
    "            # reward_matrix_random_mix.append(np.dot(reward_dictionary[pair][rho][seed][\"random_mix\"], discounter))\n",
    "\n",
    "            discounter = np.array([gamma**i for i in range(len(reward_dictionary[pair][rho][seed][\"sum\"]))])\n",
    "            reward_matrix_optimal_sum.append(np.cumsum(np.multiply(reward_dictionary[pair][rho][seed][\"sum\"], discounter)))\n",
    "            reward_matrix_optimal_mix.append(np.cumsum(np.multiply(reward_dictionary[pair][rho][seed][\"mix\"], discounter)))\n",
    "            reward_matrix_random_sum.append(np.cumsum(np.multiply(reward_dictionary[pair][rho][seed][\"random_sum\"], discounter)))\n",
    "            reward_matrix_random_mix.append(np.cumsum(np.multiply(reward_dictionary[pair][rho][seed][\"random_mix\"], discounter)))\n",
    "\n",
    "            # reward_matrix_optimal_sum.append(np.cumsum(reward_dictionary[pair][rho][seed][\"sum\"]))\n",
    "            # reward_matrix_optimal_mix.append(np.cumsum(reward_dictionary[pair][rho][seed][\"mix\"]))\n",
    "\n",
    "            # reward_matrix_random_sum.append(np.cumsum(reward_dictionary[pair][rho][seed][\"random_sum\"]))\n",
    "            # reward_matrix_random_mix.append(np.cumsum(reward_dictionary[pair][rho][seed][\"random_mix\"]))\n",
    "\n",
    "        reward_matrix_optimal_sum = np.array(reward_matrix_optimal_sum).mean(axis=0)\n",
    "        reward_matrix_optimal_mix = np.array(reward_matrix_optimal_mix).mean(axis=0)\n",
    "        reward_matrix_random_sum = np.array(reward_matrix_random_sum).mean(axis=0)\n",
    "        reward_matrix_random_mix = np.array(reward_matrix_random_mix).mean(axis=0)\n",
    "\n",
    "\n",
    "        ax_sum[i][j].plot(reward_matrix_optimal_sum,  color=\"blue\", label=\"f{}\".format(seed))\n",
    "        ax_sum[i][j].plot(reward_matrix_random_sum, color=\"orange\", label=\"f{}\".format(seed))\n",
    "        ax_mix[i][j].plot(reward_matrix_optimal_mix, color=\"blue\", label=\"f{}\".format(seed))\n",
    "        ax_mix[i][j].plot(reward_matrix_random_mix, color=\"orange\", label=\"f{}\".format(seed))\n",
    "\n",
    "\n",
    "fig_sum.legend([blue_patch, orange_patch], [\"reward_matrix_optimal\", \"reward_matrix_random\"])\n",
    "fig_mix.legend([blue_patch, orange_patch], [\"reward_matrix_optimal\", \"reward_matrix_random\"])\n",
    "fig_sum.suptitle(\"sum\")\n",
    "fig_mix.suptitle(\"mix\")\n",
    "\n",
    "fig_sum.tight_layout()\n",
    "fig_mix.tight_layout()\n",
    "\n",
    "fig_sum.savefig(\"reward_matrix_baseline_sum.png\")\n",
    "fig_mix.savefig(\"reward_matrix_baseline_mix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(16, 12), sharex=True, sharey=True)\n",
    "\n",
    "for i, pair in enumerate(result_dict):\n",
    "    for j, rho in enumerate(result_dict[pair]):\n",
    "        ax[i][j].set_title(\"team size {}, n_teams {}, rho {}\".format(pair[0], pair[1], rho))\n",
    "        ax[i][j].set_xlabel(\"seed\")\n",
    "        ax[i][j].set_ylabel(\"log of normalized timescale\")\n",
    "\n",
    "        timescales = []\n",
    "        for seed in result_dict[pair][rho]:\n",
    "            timescales.append(len(result_dict[pair][rho][seed][\"het\"][2]))\n",
    "            # ax[i][j].plot(result_dict[pair][rho][seed][\"het\"][2], label=\"f{}\".format(seed))\n",
    "            # ax_het[i][j].plot(result_dict[pair][rho][seed][\"het\"][2], label=\"f{}\".format(seed))\n",
    "            # ax[i][j].legend()\n",
    "            # ax_het[i][j].legend()\n",
    "        timescales.sort()\n",
    "        normalized_timescales = [(t-timescales[0])/(timescales[-1] - timescales[0]) for t in timescales]\n",
    "        log_norm_t = [np.log(t) for t in normalized_timescales]\n",
    "        # print(timescales)\n",
    "        print(normalized_timescales)\n",
    "        ax[i][j].plot(log_norm_t)\n",
    "\n",
    "\n",
    "# fig.suptitle(\"homogeneous\")\n",
    "# fig_het.suptitle(\"heterogeneous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bit Flip Environment (Old One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MABitFlipEnv:\n",
    "\n",
    "    def __init__(self, n_actions, n_agents, n_memory, n_teams, avg_pairwise_correlation = 0.5, is_het=False):\n",
    "        self.nA = n_actions\n",
    "        self.n_agents = n_agents\n",
    "        self.n_memory = n_memory\n",
    "        self.nS = self.nA ** (self.n_agents * self.n_memory)\n",
    "        self.nS_plus = self.nA * self.nS\n",
    "        self.state = None\n",
    "        self.n_teams = n_teams\n",
    "        self.is_het = is_het\n",
    "        self.avg_pairwise_correlation = avg_pairwise_correlation\n",
    "        self.policies = np.zeros((self.n_agents, self.nS),dtype=bool)\n",
    "        self.joint_action_seqs = []\n",
    "        self.calculate_policies()\n",
    "\n",
    "    def get_hom_policies(self):\n",
    "        team_size = int(self.n_agents/self.n_teams)\n",
    "        rng = np.random.default_rng(12345)\n",
    "        agent_indices_bool = np.zeros(self.n_agents,dtype=bool)\n",
    "        for team in range(self.n_teams):\n",
    "            agent_indices = range(team * team_size, (team+1) * team_size)\n",
    "            agent_indices_bool[agent_indices]=True\n",
    "            #joint actions for a group are assigned Bernoulli: {as same over the group, else random}\n",
    "            is_same = (self.avg_pairwise_correlation > rng.random(self.nS)) #TODO: add more than binary ations is_same=(avg_pairwise_correlation>np.random.rand(0, n_actions, n_states)) #joint actions for a group are assigned as same over the group, else random\n",
    "            n_same = np.sum(is_same)\n",
    "            n_diff = self.nS-n_same\n",
    "            self.policies[np.ix_(agent_indices_bool, is_same)] = np.random.randint(0, self.nA, n_same)[np.newaxis,:]\n",
    "            self.policies[np.ix_(agent_indices_bool, ~is_same)] = np.random.randint(0, self.nA, [team_size,n_diff])\n",
    "            agent_indices_bool[agent_indices] = False #reset\n",
    "\n",
    "    def get_het_policies(self):\n",
    "        team_size = int(self.n_agents/self.n_teams)\n",
    "        rng = np.random.default_rng(12345)\n",
    "        agent_indices_bool = np.zeros(self.n_agents, dtype=bool)\n",
    "        rho=np.sin(np.pi/2*self.avg_pairwise_correlation)\n",
    "        for team in range(self.n_teams):\n",
    "            agent_indices = range(team * team_size, (team+1) * team_size)\n",
    "            agent_indices_bool[agent_indices]=True\n",
    "            self.policies[agent_indices_bool, :] = ((np.sqrt(1 - rho)*rng.normal(size = (team_size, self.nS)) + np.sqrt(rho)*rng.normal(size = self.nS)[np.newaxis,:]) > 0)\n",
    "            agent_indices_bool[agent_indices] = False\n",
    "\n",
    "    def calculate_policies(self):\n",
    "        if self.is_het:\n",
    "            self.get_het_policies()\n",
    "        else:\n",
    "            self.get_hom_policies()\n",
    "\n",
    "    def get_reward(self, agent_index, s_plus):\n",
    "        fraction = s_plus.sum()/len(s_plus)\n",
    "        reward = 1.0 / (fraction*int(s_plus[agent_index]) + (1-fraction)*int(1-s_plus[agent_index]))\n",
    "        return reward\n",
    "\n",
    "    def P(self, s, a):\n",
    "        next_state = self.policies[:, s]\n",
    "        next_s_plus = np.insert(next_state, 0, a)\n",
    "        prob = 1.0 # transitions are deterministic\n",
    "        reward = self.get_reward(0, next_s_plus) # get reward for agent 0\n",
    "        return prob, next_s_plus, reward\n",
    "    \n",
    "    def step(self, a):\n",
    "        next_state = self.policies[:, self.state]\n",
    "        self.state = binary2index(next_state)\n",
    "        next_s_plus = np.insert(next_state, 0, a)\n",
    "        reward = self.get_reward(0, next_s_plus)\n",
    "        return next_s_plus, reward\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        # return state 0 by deafult\n",
    "        self.state = 0\n",
    "        return self.state"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "519c3646e46e87a9b4521f30b0c71a5bab07601a45b52f63f01adb46cf5a2090"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('marl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
