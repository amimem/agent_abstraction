{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import pylab as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bit Flip Environment (Old One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MABitFlipEnv:\n",
    "\n",
    "    def __init__(self, n_actions, n_agents, n_memory, n_teams, avg_pairwise_correlation = 0.5, is_het=False):\n",
    "        self.nA = n_actions\n",
    "        self.n_agents = n_agents\n",
    "        self.n_memory = n_memory\n",
    "        self.nS = self.nA ** (self.n_agents * self.n_memory)\n",
    "        self.nS_plus = self.nA * self.nS\n",
    "        self.state = None\n",
    "        self.n_teams = n_teams\n",
    "        self.is_het = is_het\n",
    "        self.avg_pairwise_correlation = avg_pairwise_correlation\n",
    "        self.policies = np.zeros((self.n_agents, self.nS),dtype=bool)\n",
    "        self.joint_action_seqs = []\n",
    "        self.calculate_policies()\n",
    "\n",
    "    def get_hom_policies(self):\n",
    "        team_size = int(self.n_agents/self.n_teams)\n",
    "        rng = np.random.default_rng(12345)\n",
    "        agent_indices_bool = np.zeros(self.n_agents,dtype=bool)\n",
    "        for team in range(self.n_teams):\n",
    "            agent_indices = range(team * team_size, (team+1) * team_size)\n",
    "            agent_indices_bool[agent_indices]=True\n",
    "            #joint actions for a group are assigned Bernoulli: {as same over the group, else random}\n",
    "            is_same = (self.avg_pairwise_correlation > rng.random(self.nS)) #TODO: add more than binary ations is_same=(avg_pairwise_correlation>np.random.rand(0, n_actions, n_states)) #joint actions for a group are assigned as same over the group, else random\n",
    "            n_same = np.sum(is_same)\n",
    "            n_diff = self.nS-n_same\n",
    "            self.policies[np.ix_(agent_indices_bool, is_same)] = np.random.randint(0, self.nA, n_same)[np.newaxis,:]\n",
    "            self.policies[np.ix_(agent_indices_bool, ~is_same)] = np.random.randint(0, self.nA, [team_size,n_diff])\n",
    "            agent_indices_bool[agent_indices] = False #reset\n",
    "\n",
    "    def get_het_policies(self):\n",
    "        team_size = int(self.n_agents/self.n_teams)\n",
    "        rng = np.random.default_rng(12345)\n",
    "        agent_indices_bool = np.zeros(self.n_agents, dtype=bool)\n",
    "        rho=np.sin(np.pi/2*self.avg_pairwise_correlation)\n",
    "        for team in range(self.n_teams):\n",
    "            agent_indices = range(team * team_size, (team+1) * team_size)\n",
    "            agent_indices_bool[agent_indices]=True\n",
    "            self.policies[agent_indices_bool, :] = ((np.sqrt(1 - rho)*rng.normal(size = (team_size, self.nS)) + np.sqrt(rho)*rng.normal(size = self.nS)[np.newaxis,:]) > 0)\n",
    "            agent_indices_bool[agent_indices] = False\n",
    "\n",
    "    def calculate_policies(self):\n",
    "        if self.is_het:\n",
    "            self.get_het_policies()\n",
    "        else:\n",
    "            self.get_hom_policies()\n",
    "\n",
    "    def get_reward(self, agent_index, s_plus):\n",
    "        fraction = s_plus.sum()/len(s_plus)\n",
    "        reward = 1.0 / (fraction*int(s_plus[agent_index]) + (1-fraction)*int(1-s_plus[agent_index]))\n",
    "        return reward\n",
    "\n",
    "    def P(self, s, a):\n",
    "        next_state = self.policies[:, s]\n",
    "        next_s_plus = np.insert(next_state, 0, a)\n",
    "        prob = 1.0 # transitions are deterministic\n",
    "        reward = self.get_reward(0, next_s_plus) # get reward for agent 0\n",
    "        return prob, next_s_plus, reward\n",
    "    \n",
    "    def step(self, a):\n",
    "        next_state = self.policies[:, self.state]\n",
    "        self.state = binary2index(next_state)\n",
    "        next_s_plus = np.insert(next_state, 0, a)\n",
    "        reward = self.get_reward(0, next_s_plus)\n",
    "        return next_s_plus, reward\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        # return state 0 by deafult\n",
    "        self.state = 0\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BF:\n",
    "    def __init__(self, n_actions, n_teams, team_size, policy_seed, avg_pairwise_correlation = 0.5, is_het=False):\n",
    "        self.nA = n_actions\n",
    "        self.n_teams = n_teams\n",
    "        self.team_size = team_size\n",
    "        self.all_agents = n_teams * team_size + 1\n",
    "        self.nS = self.nA ** (self.all_agents)\n",
    "        self.policy_seed = policy_seed\n",
    "        self.corr = avg_pairwise_correlation\n",
    "        self.is_het = is_het\n",
    "        self.policies = None\n",
    "        self.calculate_policies()\n",
    "\n",
    "    def get_mix_policies(self):\n",
    "        n_agents = self.n_teams * self.team_size\n",
    "        n_states = self.nA ** (n_agents + 1)  # add states of agent 1\n",
    "        self.policies = np.zeros((n_agents, n_states), dtype=bool)\n",
    "\n",
    "        rng = np.random.default_rng(self.policy_seed)\n",
    "        agent_indices_bool = np.zeros(n_agents, dtype=bool)\n",
    "\n",
    "        for team in range(self.n_teams):\n",
    "            agent_indices = range(team * self.team_size, (team + 1) * self.team_size)\n",
    "            agent_indices_bool[agent_indices] = True\n",
    "            # joint actions for a group are assigned Bernoulli: {as same over the group, else random}\n",
    "            is_same = self.corr > rng.random(n_states)\n",
    "            n_same = np.sum(is_same)\n",
    "            n_diff = n_states - n_same\n",
    "            self.policies[np.ix_(agent_indices_bool, is_same)] = rng.integers(\n",
    "                0, self.nA, n_same\n",
    "            )[np.newaxis, :]\n",
    "            self.policies[np.ix_(agent_indices_bool, ~is_same)] = rng.integers(\n",
    "                0, self.nA, [self.team_size, n_diff]\n",
    "            )\n",
    "            agent_indices_bool[agent_indices] = False\n",
    "        # self.policies = policies\n",
    "        # return policies\n",
    "\n",
    "\n",
    "    def get_sum_policies(self):\n",
    "        n_agents = n_teams * self.team_size\n",
    "        n_states = self.nA ** (n_agents + 1)  # add states of agent 1\n",
    "        self.policies = np.zeros((n_agents, n_states), dtype=bool)\n",
    "\n",
    "        rng = np.random.default_rng(self.policy_seed)\n",
    "        agent_indices_bool = np.zeros(n_agents, dtype=bool)\n",
    "\n",
    "        rho = np.sin(np.pi / 2 * self.corr)\n",
    "        for team in range(n_teams):\n",
    "            agent_indices = range(team * self.team_size, (team + 1) * self.team_size)\n",
    "            agent_indices_bool[agent_indices] = True\n",
    "            self.policies[agent_indices_bool, :] = (\n",
    "                np.sqrt(1 - rho) * rng.normal(size=(self.team_size, n_states))\n",
    "                + np.sqrt(rho) * rng.normal(size=n_states)[np.newaxis, :]\n",
    "            ) > 0\n",
    "            agent_indices_bool[agent_indices] = False\n",
    "        # self.policies = policies\n",
    "        # return policies\n",
    "\n",
    "    def calculate_policies(self):\n",
    "        if self.is_het:\n",
    "            self.get_mix_policies()\n",
    "        else:\n",
    "            self.get_sum_policies()\n",
    "\n",
    "    def get_reward(self, agent_index, state):\n",
    "        fraction = state.sum()/len(state)\n",
    "        reward = 1.0 / (fraction*int(state[agent_index]) + (1-fraction)*int(1-state[agent_index]))\n",
    "        return reward\n",
    "\n",
    "    def P(self, s, a):\n",
    "        next_state = self.policies[:, s]\n",
    "        # print(next_state)\n",
    "        next_state[-1] = a\n",
    "        # print(\"state, next state with action is \")\n",
    "        # print(a, next_state)\n",
    "        prob = 1.0 # transitions are deterministic\n",
    "        reward = self.get_reward(agent_index=-1, state=next_state) # get reward for agent 0\n",
    "        return prob, next_state, reward\n",
    "\n",
    "    def step(self, a):\n",
    "        next_state = self.policies[:, self.state]\n",
    "        self.state = binary2index(next_state)\n",
    "        next_s_plus = np.insert(next_state, 0, a)\n",
    "        reward = self.get_reward(0, next_s_plus)\n",
    "        return next_s_plus, reward\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        # return state 0 by deafult\n",
    "        self.state = 0\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary2index(var):\n",
    "    return np.sum([2**n for n in range(len(var))] * var.flatten()).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0, max_iterations=100000):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward in [env.P(state, a)]:\n",
    "                next_state = binary2index(next_state)\n",
    "                # print(\"next state index is \", next_state)\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    time_step = 0\n",
    "    \n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        time_step += 1\n",
    "        delta = 0\n",
    "\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value        \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            print(\"converged\", time_step)\n",
    "            break\n",
    "        if time_step >= max_iterations:\n",
    "            print(\"max iterations reached\", time_step)\n",
    "            break\n",
    "\n",
    "        if time_step % 100 == 0:\n",
    "            print(\"time step \", time_step)\n",
    "            # print(V[:10])\n",
    "\n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros(env.nS, dtype='int')\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        # A[0] corresponds to action 0 and A[1] corresponds to action 1, therefore instead of the one-hot value we use the index of the best action\n",
    "        policy[s] = best_action\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:  2 2 0.0 het:  True\n",
      "time step  100\n",
      "time step  200\n",
      "time step  300\n",
      "time step  400\n",
      "time step  500\n",
      "converged 532\n",
      "params:  2 2 0.0 het:  False\n",
      "time step  100\n",
      "time step  200\n",
      "time step  300\n",
      "time step  400\n",
      "converged 452\n",
      "params:  5 2 0.0 het:  True\n",
      "time step  100\n",
      "time step  200\n",
      "time step  300\n",
      "time step  400\n",
      "time step  500\n",
      "converged 505\n",
      "params:  5 2 0.0 het:  False\n",
      "time step  100\n",
      "time step  200\n",
      "time step  300\n",
      "time step  400\n",
      "converged 475\n",
      "params:  2 2 0.5 het:  True\n",
      "time step  100\n",
      "time step  200\n",
      "time step  300\n",
      "time step  400\n",
      "time step  500\n",
      "time step  600\n",
      "time step  700\n",
      "converged 711\n",
      "params:  2 2 0.5 het:  False\n",
      "time step  100\n",
      "time step  200\n",
      "time step  300\n",
      "time step  400\n",
      "time step  500\n",
      "converged 547\n",
      "params:  5 2 0.5 het:  True\n",
      "time step  100\n",
      "time step  200\n",
      "time step  300\n",
      "time step  400\n",
      "time step  500\n",
      "time step  600\n",
      "converged 645\n",
      "params:  5 2 0.5 het:  False\n",
      "time step  100\n",
      "time step  200\n",
      "time step  300\n",
      "time step  400\n",
      "time step  500\n",
      "time step  600\n",
      "time step  700\n",
      "time step  800\n",
      "time step  900\n",
      "converged 949\n",
      "params:  2 2 1.0 het:  True\n",
      "time step  100\n",
      "time step  200\n",
      "time step  300\n",
      "time step  400\n",
      "time step  500\n",
      "time step  600\n",
      "time step  700\n",
      "time step  800\n",
      "time step  900\n",
      "time step  1000\n",
      "converged 1039\n",
      "params:  2 2 1.0 het:  False\n",
      "time step  100\n",
      "time step  200\n",
      "time step  300\n",
      "time step  400\n",
      "time step  500\n",
      "converged 547\n",
      "params:  5 2 1.0 het:  True\n",
      "time step  100\n",
      "time step  200\n",
      "time step  300\n",
      "time step  400\n",
      "time step  500\n",
      "converged 582\n",
      "params:  5 2 1.0 het:  False\n",
      "time step  100\n",
      "time step  200\n",
      "time step  300\n",
      "time step  400\n",
      "time step  500\n",
      "time step  600\n",
      "time step  700\n",
      "time step  800\n",
      "time step  900\n",
      "time step  1000\n",
      "time step  1100\n",
      "converged 1147\n"
     ]
    }
   ],
   "source": [
    "for rho in [0.0, 0.5, 1.0]:\n",
    "    for team_size, n_teams in [(2,2), (5,2)]:\n",
    "        env_het = BF(n_actions=2, n_teams=n_teams, team_size=team_size, avg_pairwise_correlation=rho, policy_seed=3, is_het=True)\n",
    "        env_hom = BF(n_actions=2, n_teams=n_teams, team_size=team_size, avg_pairwise_correlation=rho, policy_seed=3, is_het=False)\n",
    "        print(\"params: \", team_size, n_teams, rho, \"het: \", env_het.is_het)\n",
    "        policy_het, V_het = value_iteration(env_het, discount_factor=0.99, max_iterations=10000)\n",
    "        print(\"params: \", team_size, n_teams, rho, \"het: \", env_hom.is_het)\n",
    "        policy_hom, V_hom = value_iteration(env_hom, discount_factor=0.99,max_iterations=10000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "519c3646e46e87a9b4521f30b0c71a5bab07601a45b52f63f01adb46cf5a2090"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('marl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
